---
layout: post
title:  Conference on robustness and privacy
comments: true
date:   2021-02-17
---

We are organizing a two days virtual conference on "robustness and privacy" on March 22 and March 23, 2021.   

The registration is free but mandatory; a Zoom link will be sent to all participants a day before the conference

[<center><font face="verdana" size='5' color='blue'> register here </font></center>](https://docs.google.com/forms/d/e/1FAIpQLSf_yZApHaZVZ791DXR-sAP--R_bnkeJ6EEKr_F0qNDrWFmUiw/viewform?vc=0&c=0&w=1&flr=0&usp=mail_form_link)




Each talk will be 25 minutes long followed by a 5 minutes dicussion for questions. The conference should start on Monday 22th of March and Tuesday 23rd of March at 16:30 up to 19h30 (Paris local time).


**Confirmed speakers:**



[<center><font face="verdana" size='5' color='red'> Marco Avella Medina </font></center>](https://sites.google.com/site/marcoavellamedina/home)
**Title: Differentially private inference via noisy optimization**

*Abstract:* We propose a general optimization-based framework for computing differentially private M-estimators and  a new method for the construction of differentially private confidence regions. Firstly, we show that robust statistics can be used in conjunction with noisy gradient descent and noisy Newton methods in order to obtain optimal private estimators with global linear or quadratic convergence, respectively. We establish global convergence guarantees, under both local strong convexity and self-concordance, showing that our private estimators converge with high probability to a neighborhood of the non-private M-estimators. The radius of this neighborhood is nearly optimal in the sense it corresponds to the statistical minimax cost of differential privacy up to a logarithmic term. Secondly, we tackle the problem of parametric inference by constructing differentially private estimators of the asymptotic variance of our private M-estimators. This naturally leads to the use of approximate pivotal statistics for the construction of confidence regions and hypothesis testing. We demonstrate the effectiveness of a bias correction that leads to enhanced small-sample empirical performance in simulations. We illustrate the benefits of our methods with synthetic numerical examples and real data.

This is joint work with Casey Bradshaw and Po-Ling Loh.

[<center><font face="verdana" size='5' color='red'> Tom Berrett </font></center>](https://thomasberrett.github.io) 
**TBA**
[<center><font face="verdana" size='5' color='red'> Clément Canonne </font></center>](https://ccanonne.github.io)
**Title : Lower bounds for high-dimensional estimation under "local" information constraints**

*abstract :* The focus of this talk is distributed parameter estimation using
interactive protocols subject to "local information constraints."
Those constraints include, among others local differential privacy
(LDP), communication constraints, and restricted measurements.

I'll cover recent work which provides a general framework which lets
us obtain tight lower bounds for both estimation and testing of
discrete distributions. I'll then discuss how to extend this framework
to other parametric families, including mean estimation for product
distributions over the hypercube and high-dimensional Gaussians.

Based on joint works with Jayadev Acharya, Yuhan Liu, Ziteng Sun, and
Himanshu Tyagi.

links to [paper 1](https://arxiv.org/abs/2007.10976) and [paper 2](https://arxiv.org/abs/2010.06562)


[<center><font face="verdana" size='5' color='red'> Olivier Catoni </font></center>](http://ocatoni.perso.math.cnrs.fr)
**TBA**
[<center><font face="verdana" size='5' color='red'> Yeshwanth Cherapanamjeri </font></center>](https://yeshwanth94.github.io) 
**TBA**
[<center><font face="verdana" size='5' color='red'> John Duchi </font></center>](https://web.stanford.edu/~jduchi/) 
**TBA**
[<center><font face="verdana" size='5' color='red'> Chao Gao </font></center>](https://www.stat.uchicago.edu/~chaogao/) 
**TBA**
[<center><font face="verdana" size='5' color='red'> Sam Hopkins </font></center>](https://www.samuelbhopkins.com) 
**TBA**
[<center><font face="verdana" size='5' color='red'> Gautam Kamath </font></center>](http://www.gautamkamath.com/) 
**Title: Differentially Private Mean and Covariance Estimation**

*Abstract:* We cover recent progress in minimax rates for differentially private parameter estimation. As a canonical example, how much data do we need to estimate the parameters of a multivariate Gaussian distribution? A particular focus will be placed on qualitative differences with the non-private setting.

Links to [paper 1](https://arxiv.org/abs/1805.00216) and [paper 2](https://arxiv.org/abs/2002.09464)

[<center><font face="verdana" size='5' color='red'> Pavlo Mozharovskyi </font></center>](https://perso.telecom-paristech.fr/mozharovskyi/)
**Title: Approximate computation of projection depths**

*Abstract:* Data depth is a concept in multivariate statistics that measures the centrality of a point in a given data cloud in the Euclidean space. If the depth of a point can be represented as the minimum of the depths with respect to all one-dimensional projections of the data, then the depth satisfies the so-called projection property. Such depths form an important class that includes many of the depths that have been proposed in literature. For depths that satisfy the projection property an approximate algorithm can easily be constructed since taking the minimum of the depths with respect to only a finite number of one-dimensional projections yields an upper bound for the depth with respect to the multivariate data. Such an algorithm is particularly useful if no exact algorithm exists or if the exact algorithm has a high computational complexity, as is the case with the halfspace depth or the projection depth. To compute these depths in high dimensions, the use of an approximate algorithm with better complexity is surely preferable. Instead of focusing on a single method we provide a comprehensive and fair comparison of several methods, both already described in the literature and original.

Link to [paper](https://arxiv.org/abs/2007.08016)

[<center><font face="verdana" size='5' color='red'> Weije Su </font></center>](https://statistics.wharton.upenn.edu/profile/suw/) 
**TBA**




<!-- [<center><font face="verdana" size='5' color='red'>  </font></center>]() 
 -->

<!-- Sabato, Gao, Minsker, --> 

<!-- Daniel Kane --> 

<!-- Amandine Dubois -->

**Schedule (Paris local time)**

<center>

<table>
  <tr>
    <th>   </th>
    <th style="background-color: yellow">Monday 22 March</th>
    <th style="background-color: red">Tuesday 23 March</th>
  </tr>
  <tr>
    <td>16:30 -- 17:00</td>
    <td style="background-color: yellow">Olivier Catoni</td>
    <th style="background-color: red">Weije Su</th>
  </tr>
  <tr>
    <td>17:00 -- 17:30</td>
    <td style="background-color: yellow">Samuel Hopkins</td>
    <th style="background-color: red">Gautam Kamath </th>
  </tr>
    <tr>
    <td>17:30 -- 18:00</td>
    <td style="background-color: yellow">Pavlo Mozharovskyi</td>
    <th style="background-color: red">  Clément Canonne </th>
  </tr>
    <tr>
    <td>18:00 -- 18:30</td>
    <td style="background-color: yellow">Tom Berrett </td>
    <th style="background-color: red"> Yeshwanth Cherapanamjeri  </th>
  </tr>
    <tr>
    <td>18:30 -- 19:00</td>
    <td style="background-color: yellow">Marco Avella Medina</td>
    <th style="background-color: red">  Chao Gao </th>
  </tr>
    <tr>
    <td>19:00 -- 19:30</td>
    <td style="background-color: yellow">John Duchi</td>
    <th style="background-color: red">   </th>
  </tr>
</table>


</center>



    

**Support**
The conference is suported by the mathematical institute of CNRS and the Médiamétrie Chair. 

**Some words about the conference:**

**Robustness** has been studied a lot in statistics since the seminal work of Huber. Many algorithms have also been designed in Machine Learning. However, this subject has witnessed an important renew  during the last 10 years both in the statistical and computer science communities. It involves statistics, optimization, probability and machine learning as mathematical domains. 

In the meantime, **privacy** has received a lot of attention in the Computer science community because it is a central issue for the security of many sensitive data in finance, economics, administration and, more basically, for keeping customers' trust in trading. It seems however possible to randomize data in order to ensure a controlled amount of privacy of the individuals while still being able to learn some patterns out of it: this is the cornerstone of privacy. Only  recently, statisticians started to analyze privacy mechanisms. In particular, they discovered several interesting common features between robustness issues and privacy. That is the main motivation on our side to organize a joint conference simultaneously on the two subjects. 

**Organization commitee:**
Cristina Butucea, Victor-Emmanuel Brunel, Nicolas Chopin,  Arnak Dalalyan, Guillaume Lecué, Matthieu Lerasle, Vianney Perchet and Alexandre Tsybakov.

**Info contact** You can email [me](https://lecueguillaume.github.io/about/) for more information.




 