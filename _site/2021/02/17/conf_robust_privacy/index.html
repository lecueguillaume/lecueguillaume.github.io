<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Conference on robustness and privacy</title>
  <meta name="description" content="We are organizing a two days virtual conference on “robustness and privacy” on March 22 and March 23, 2021.">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/2021/02/17/conf_robust_privacy/">
  <link rel="alternate" type="application/rss+xml" title="Homepage of Guillaume Lecué" href="http://localhost:4000/feed.xml" />
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Homepage of Guillaume Lecué</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About me</a>
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/research/">Research</a>
          
        
          
          <a class="page-link" href="/teaching/">Teaching</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Conference on robustness and privacy</h1>
    <p class="post-meta">Feb 17, 2021</p>
  </header>

  <article class="post-content">
    <p>We are organizing a two days virtual conference on “robustness and privacy” on March 22 and March 23, 2021.</p>

<p>The registration is free but mandatory; a Zoom link will be sent to all participants a day before the conference</p>

<p><a href="https://docs.google.com/forms/d/e/1FAIpQLSf_yZApHaZVZ791DXR-sAP--R_bnkeJ6EEKr_F0qNDrWFmUiw/viewform?vc=0&amp;c=0&amp;w=1&amp;flr=0&amp;usp=mail_form_link"><center><font face="verdana" size="5" color="blue"> register here </font></center></a></p>

<p>Each talk will be 25 minutes long followed by a 5 minutes dicussion for questions. The conference should start on Monday 22th of March and Tuesday 23rd of March at 16:30 up to 19h30 (Paris local time).</p>

<p><strong>Confirmed speakers:</strong></p>

<p><a href="https://sites.google.com/site/marcoavellamedina/home"><center><font face="verdana" size="5" color="red"> Marco Avella Medina </font></center></a>
<strong>Title: Differentially private inference via noisy optimization</strong></p>

<p><em>Abstract:</em> We propose a general optimization-based framework for computing differentially private M-estimators and  a new method for the construction of differentially private confidence regions. Firstly, we show that robust statistics can be used in conjunction with noisy gradient descent and noisy Newton methods in order to obtain optimal private estimators with global linear or quadratic convergence, respectively. We establish global convergence guarantees, under both local strong convexity and self-concordance, showing that our private estimators converge with high probability to a neighborhood of the non-private M-estimators. The radius of this neighborhood is nearly optimal in the sense it corresponds to the statistical minimax cost of differential privacy up to a logarithmic term. Secondly, we tackle the problem of parametric inference by constructing differentially private estimators of the asymptotic variance of our private M-estimators. This naturally leads to the use of approximate pivotal statistics for the construction of confidence regions and hypothesis testing. We demonstrate the effectiveness of a bias correction that leads to enhanced small-sample empirical performance in simulations. We illustrate the benefits of our methods with synthetic numerical examples and real data.</p>

<p>This is joint work with Casey Bradshaw and Po-Ling Loh.</p>

<p><a href="https://thomasberrett.github.io"><center><font face="verdana" size="5" color="red"> Tom Berrett </font></center></a></p>

<p><strong>Title: Locally private non-asymptotic testing of discrete distributions is faster using interactive mechanisms</strong></p>

<p><em>abstract:</em> In this talk I will present recent work on goodness-of-fit testing under local differential privacy constraints. There are two broad classes of locally private procedures: simple non-interactive procedures and interactive procedures where communication is allowed between individual data holders. One of the main conclusions of our work is that the minimax separation rates are significantly faster using interactive mechanisms.</p>

<p>We find the minimax separation rates for testing multinomial or more general discrete distributions. Our upper bounds are found by constructing efficient randomized algorithms and test procedures, in both the case where only non-interactive privacy mechanisms are allowed and also in the case where all sequentially interactive privacy mechanisms are allowed, and establish a gap between the rates. We prove general information theoretical bounds that allow us to establish the optimality of our algorithms among all pairs of privacy mechanisms and test procedures, in most usual cases. Considered examples include testing uniform, polynomially and exponentially decreasing distributions.</p>

<p>link to  <a href="https://papers.nips.cc/paper/2020/hash/20b02dc95171540bc52912baf3aa709d-Abstract.html">paper</a></p>

<p><a href="https://ccanonne.github.io"><center><font face="verdana" size="5" color="red"> Clément Canonne </font></center></a>
<strong>Title: Lower bounds for high-dimensional estimation under “local” information constraints</strong></p>

<p><em>abstract:</em> The focus of this talk is distributed parameter estimation using interactive protocols subject to “local information constraints.”
Those constraints include, among others local differential privacy (LDP), communication constraints, and restricted measurements.</p>

<p>I’ll cover recent work which provides a general framework which lets us obtain tight lower bounds for both estimation and testing of discrete distributions. I’ll then discuss how to extend this framework to other parametric families, including mean estimation for product distributions over the hypercube and high-dimensional Gaussians.</p>

<p>Based on joint works with Jayadev Acharya, Yuhan Liu, Ziteng Sun and Himanshu Tyagi.</p>

<p>links to <a href="https://arxiv.org/abs/2007.10976">paper 1</a> and <a href="https://arxiv.org/abs/2010.06562">paper 2</a></p>

<p><a href="http://ocatoni.perso.math.cnrs.fr"><center><font face="verdana" size="5" color="red"> Olivier Catoni </font></center></a>
<strong>TBA</strong>
<a href="https://yeshwanth94.github.io"><center><font face="verdana" size="5" color="red"> Yeshwanth Cherapanamjeri </font></center></a> 
<strong>TBA</strong>
<a href="https://julesdepersin.github.io"><center><font face="verdana" size="5" color="red"> Jules Depersin </font></center></a> 
<strong>Title: Using VC-dimension in robust estimation</strong></p>

<p><em>abstract:</em> Median-of-means (MOM) based procedures provide non-asymptotic and strong deviation bounds even when data are heavy-tailed and/or corrupted. We will try to explain how those procedures can easily be adapted to a number of different statistical problems, and we will give the general ways to bound the excess risk for MOM estimators, with an emphasis on the different notions of statistical complexity (VC dimension, Rademacher complexity) that can be used to do so. We will see that each classical notion of complexity is suboptimal in its own way, leaving open the question of the optimal way to measure the statistical complexity of robust estimation problems.</p>

<p>Link to <a href="https://arxiv.org/abs/2004.11734">paper</a></p>

<p><a href="https://web.stanford.edu/~jduchi/"><center><font face="verdana" size="5" color="red"> John Duchi </font></center></a> 
<strong>TBA</strong></p>

<p><a href="https://www.stat.uchicago.edu/~chaogao/"><center><font face="verdana" size="5" color="red"> Chao Gao </font></center></a> 
<strong>title: Robust Regression with Contamination</strong></p>

<p><em>abstract:</em> We study regression with contaminated observations. We will discuss different results depending on whether both the responses and the covariates are contaminated or only the responses are contaminated. General minimax rates are derived based on regression depth functions when both the responses and the covariates are contaminated. The result implies consistency is possible only if the contamination proportion is vanishing. In comparison, we show that when the covariates are clean, consistent robust regression is actually possible even when the contamination proportion approaches one. A near-optimal procedure in this case is the simple median regression. Applications of the second setting in model repair problems will also be discussed.</p>

<p><a href="https://www.samuelbhopkins.com"><center><font face="verdana" size="5" color="red"> Sam Hopkins </font></center></a> 
<strong>Title: Robustly Learning any Clusterable Mixture of Gaussians</strong></p>

<p><em>Abstract:</em> We study the efficient learnability of high-dimensional Gaussian mixtures in the outlier-robust setting, where a small constant fraction of the data is adversarially corrupted. We resolve the polynomial learnability of this problem when the components are pairwise separated in total variation distance. Specifically, we provide an algorithm that, for any constant number of components \( k \), runs in polynomial time and learns the components of an \( \epsilon \)-corrupted \( k \)-mixture within information theoretically near-optimal error of \( O(\epsilon) \), under the assumption that the overlap between any pair of components \( Pi,Pj \) (i.e., the quantity \( 1−TV(Pi,Pj)\)) is bounded by poly(\( \epsilon \))).</p>

<p>Our separation condition is the qualitatively weakest assumption under which accurate clustering of the samples is possible. In particular, it allows for components with arbitrary covariances and for components with identical means, as long as their covariances differ sufficiently. Ours is the first polynomial time algorithm for this problem, even for \( k=2 \).</p>

<p>Our algorithm follows the Sum-of-Squares based proofs to algorithms approach. Our main technical contribution is a new robust identifiability proof of clusters from a Gaussian mixture, which can be captured by the constant-degree Sum of Squares proof system. The key ingredients of this proof are a novel use of SoS-certifiable anti-concentration and a new characterization of pairs of Gaussians with small (dimension-independent) overlap in terms of their parameter distance.</p>

<p><a href="http://www.gautamkamath.com/"><center><font face="verdana" size="5" color="red"> Gautam Kamath </font></center></a> 
<strong>Title: Differentially Private Mean and Covariance Estimation</strong></p>

<p><em>Abstract:</em> We cover recent progress in minimax rates for differentially private parameter estimation. As a canonical example, how much data do we need to estimate the parameters of a multivariate Gaussian distribution? A particular focus will be placed on qualitative differences with the non-private setting.</p>

<p>Links to <a href="https://arxiv.org/abs/1805.00216">paper 1</a> and <a href="https://arxiv.org/abs/2002.09464">paper 2</a></p>

<p><a href="https://perso.telecom-paristech.fr/mozharovskyi/"><center><font face="verdana" size="5" color="red"> Pavlo Mozharovskyi </font></center></a>
<strong>Title: Approximate computation of projection depths</strong></p>

<p><em>Abstract:</em> Data depth is a concept in multivariate statistics that measures the centrality of a point in a given data cloud in the Euclidean space. If the depth of a point can be represented as the minimum of the depths with respect to all one-dimensional projections of the data, then the depth satisfies the so-called projection property. Such depths form an important class that includes many of the depths that have been proposed in literature. For depths that satisfy the projection property an approximate algorithm can easily be constructed since taking the minimum of the depths with respect to only a finite number of one-dimensional projections yields an upper bound for the depth with respect to the multivariate data. Such an algorithm is particularly useful if no exact algorithm exists or if the exact algorithm has a high computational complexity, as is the case with the halfspace depth or the projection depth. To compute these depths in high dimensions, the use of an approximate algorithm with better complexity is surely preferable. Instead of focusing on a single method we provide a comprehensive and fair comparison of several methods, both already described in the literature and original.</p>

<p>Link to <a href="https://arxiv.org/abs/2007.08016">paper</a></p>

<p><a href="https://statistics.wharton.upenn.edu/profile/suw/"><center><font face="verdana" size="5" color="red"> Weijie Su </font></center></a> 
<strong>Title:</strong> A Central Limit Theorem for Differentially Private Query Answering</p>

<p><em>Abstract:</em> Perhaps the single most important use case for differential privacy is to privately answer numerical queries, which is usually achieved by adding noise to the answer vector. The central question is, therefore, to understand which noise distribution optimizes the privacy-accuracy trade-off, especially when the dimension of the answer vector is high. Accordingly, extensive literature has been dedicated to the question and the upper and lower bounds have been matched up to constant factors. In this talk, we take a novel approach to address this important optimality question. We first demonstrate an intriguing central limit theorem phenomenon in the high-dimensional regime. More precisely, we prove that a mechanism is approximately Gaussian differentially private if the added noise satisfies certain conditions. In particular, densities proportional to $\e^{-|x|_p^\alpha}$, where $|x|_p$ is the standard $\ell_p$-norm, satisfies the conditions. Taking this perspective, we make use of the Cramer–Rao inequality and show an uncertainty principle style result: the product of privacy parameter and the $\ell_2$-loss of the mechanism is lower bounded by the dimension. Furthermore, the Gaussian mechanism achieves the constant-sharp optimal privacy-accuracy trade-off among all such noises. Our findings are corroborated by numerical experiments. This is joint work with Jinshuo Dong and Linjun Zhang.</p>

<!-- [<center><font face="verdana" size='5' color='red'>  </font></center>]() 
 -->

<!-- Sabato, Gao, Minsker, -->

<!-- Daniel Kane -->

<!-- Amandine Dubois -->

<p><strong>Schedule (Paris local time)</strong></p>

<center>

<table>
  <tr>
    <th>   </th>
    <th style="background-color: yellow">Monday 22 March</th>
    <th style="background-color: red">Tuesday 23 March</th>
  </tr>
  <tr>
    <td>16:30 -- 17:00</td>
    <td style="background-color: yellow">Olivier Catoni</td>
    <th style="background-color: red">Weijie Su</th>
  </tr>
  <tr>
    <td>17:00 -- 17:30</td>
    <td style="background-color: yellow">Samuel Hopkins</td>
    <th style="background-color: red">Gautam Kamath </th>
  </tr>
    <tr>
    <td>17:30 -- 18:00</td>
    <td style="background-color: yellow">Pavlo Mozharovskyi</td>
    <th style="background-color: red">  Clément Canonne </th>
  </tr>
    <tr>
    <td>18:00 -- 18:30</td>
    <td style="background-color: yellow">Tom Berrett </td>
    <th style="background-color: red"> Yeshwanth Cherapanamjeri  </th>
  </tr>
    <tr>
    <td>18:30 -- 19:00</td>
    <td style="background-color: yellow">Marco Avella Medina</td>
    <th style="background-color: red"> Jules Depersin  </th>
  </tr>
    <tr>
    <td>19:00 -- 19:30</td>
    <td style="background-color: yellow">John Duchi</td>
    <th style="background-color: red"> Chao Gao  </th>
  </tr>
</table>


</center>

<p><strong>Support</strong>
The conference is suported by the mathematical institute of CNRS and the Médiamétrie Chair.</p>

<p><strong>Some words about the conference:</strong></p>

<p><strong>Robustness</strong> has been studied a lot in statistics since the seminal work of Huber. Many algorithms have also been designed in Machine Learning. However, this subject has witnessed an important renew  during the last 10 years both in the statistical and computer science communities. It involves statistics, optimization, probability and machine learning as mathematical domains.</p>

<p>In the meantime, <strong>privacy</strong> has received a lot of attention in the Computer science community because it is a central issue for the security of many sensitive data in finance, economics, administration and, more basically, for keeping customers’ trust in trading. It seems however possible to randomize data in order to ensure a controlled amount of privacy of the individuals while still being able to learn some patterns out of it: this is the cornerstone of privacy. Only  recently, statisticians started to analyze privacy mechanisms. In particular, they discovered several interesting common features between robustness issues and privacy. That is the main motivation on our side to organize a joint conference simultaneously on the two subjects.</p>

<p><strong>Organization commitee:</strong>
Cristina Butucea, Victor-Emmanuel Brunel, Nicolas Chopin,  Arnak Dalalyan, Guillaume Lecué, Matthieu Lerasle, Vianney Perchet and Alexandre Tsybakov.</p>

<p><strong>Info contact</strong> You can email <a href="https://lecueguillaume.github.io/about/">me</a> for more information.</p>


  </article>

  <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>

  
  <!-- Add Disqus comments. -->
  <div id="disqus_thread"></div>
  <script type="text/javascript">
      /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
      var disqus_shortname = 'lecueguillaume'; 
      var disqus_identifier = "/2021/02/17/conf_robust_privacy/";

      /* * * DON'T EDIT BELOW THIS LINE * * */
      (function() {
          var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
          dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
          (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

  <script id="dsq-count-scr" src="//lecueguillaume.disqus.com/count.js" async></script>



</div>

      </div>
    </div>
    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Homepage of Guillaume Lecué</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Homepage of Guillaume Lecué</li>
          <li><a href="mailto:guillaume.lecue@ensae.fr">guillaume.lecue@ensae.fr</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/lecueguillaume">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">lecueguillaume</span>
            </a>
          </li>
          

          
        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">Empirical processes theory; Learning theory; Compressed sensing; aggregation; phase retreival; Minimax;</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
